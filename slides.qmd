---
title: "Counting Statistics"
subtitle: "And Non-Parametric Equivalents"
author: Rodney Dyer, PhD
format: 
  revealjs:
    echo: true
---



# Non-Parametrics {background-color="black" background-image="media/contour.png" background-size="initial" background-position="right"}



```{r echo=FALSE}
library( tidyverse )
library( ggplot2 )
library( knitr )
library( kableExtra )
```



## {background-image="media/AnalysisMatrix.png" background-size="600px, cover" background-position="center"}



# Binomial {background-color="black" background-image="media/contour.png" background-size="initial" background-position="right"}


## The Binomial

For cases where we have things that can be put into two groups.

$$
P(x=K|N,p) = \frac{N!}{ K! (N-K)!}p^K(1-p)^{N-K}
$$


:::: {.columns}

::: {.column width="50%"}
![](media/catfish.png){width=45%}
:::

::: {.column width="50%"}
Used as the *generative function* for Bayesian Inferences.
:::

::::


## The Binomial Test  {.smaller}

Can be used to test if a set of data are equivallent to an expectation given by the underlying probability of seeing an item in "Group 1" (catfish in this example):

&nbsp;


$H_O: p = \hat{p}$


&nbsp;

For this, the test statistics is defined as the number of observations in "Group 1" (e.g., $N_{catfish}$ in this example), and what we know about the probability of selecting items that have two distinct states is used to evaluate the likelihood of getting $N_{catfish}$.



## The Data {.smaller}

Borrowing from a previous example, let's assume we are sampling fish in the James River.  The hypothesis is that the frequency of catfish is 

$$
p_{catfish} = \frac{N_{catfish}}{N_{catfish} + N_{other}} = \frac{37}{50} \approx 0.74
$$

So if if we have another set of sample, we can test the null hypothesis:

$H_O: p = 0.74$

Using the new counts of $N_{catfish}$ and $N_{other}$.

## Built-In Testing Function

`R` has a direct function for this:

![](media/binomial_signature.png)
&nbsp;

That allows testing two-tailed or single-sided alternative hypotheses.


## Example With New Data

So, let's assume you went out and sampled new data and found

```{r echo=TRUE}
catfish <- 52
non_catfish <- 47 
```

::: {.fragment}

&nbsp;


To test if this sampling exercise is consistent with the previous assumption of a 74% catfish presence, we can specify the data as:


```{r}
samples <- c( catfish, non_catfish )
p_catfish <- 0.74
```

:::


## The Binomial Test


```{r}
fit <- binom.test( samples, p = p_catfish)
fit
```

<font color="red">Conclusion?</font>


## Contained Data

As normal, the object that is returned by the function has all the necessary items for you to include the data into your manuscript.

```{r}
names(fit)
```



## Multinomial Extension

When we have more than two unique states, we can extend the Binomial test to a more generalized *Multinomial* expansion.  This would be used to test the hypothesis about specific probabilities for $K-1$ states (the number of independently assorting groups).  However, this is a specific version of a more generalilzed approach using Contingency Tables and I won't be going over it here.  

However, there are a few libraries in `R` available if you'd prefer to use the multinomial expansion.


# Contingency Table Tests {background-color="black" background-image="media/contour.png" background-size="initial" background-position="right"}


## Contingency Tables {.smaller}

Contingency tables are another counting approach, where your data can be classified into one or more categories. Often it is taken as a comparison between populations.  So, in keeping with our fish theme, let's assume we went to another river and did another sampling run on catfish.  

![](media/ContingencyTable.png)

## Nomenclature

This is defined as an $RxC$ Contingency Table, where $R$ is the number of Rows and $C$ is the number of columns.  



## Underlying Hypothesis

For this, we are assuming that the observations in the row are taken from a larger background population.

- $p_{catfish,\; pop1} = \frac{O_{11}}{n_1}$
- $p_{other,\; pop1} = \frac{O_{12}}{n_1}$
- $p_{catfish} + p_{other} = 1.0$

::: {.fragment}
The same applies for samples from *Population 2*.


$H_O: p_{catfish, \; pop1} = p_{catfish, \; pop2}$

:::

## Larger Configurations

If we had more than just two Groups, we expand these to the vector of probabilities by row.

$H_O: \vec{p}_i = \vec{p}_j$

&nbsp;

And the same applies for more than just two rows.


## Test Statistic

The test statistic for this is based upon:

> The distance between the observed and expectations for the data.


:::: {.columns}

::: {.column width="50%"}
**Observed**

The count of items in each combination of categories.
 - $O_{11}$, $O_{21}$, etc.
:::

::: {.column width="50%"}
**Expected**

Based on number of categories **OR** external information.
:::
::::


## Observed Values - Example Data 

Let's assume I went out and sampled 50 more fishes and my overall data are now:

```{r}
Observed <- matrix( c(52, 47, 32, 18), nrow=2, byrow = TRUE)
rownames( Observed ) <- c("Population 1", "Population 2")
colnames( Observed ) <- c("Catfish", "Other")
Observed
```




## Defining Expectations - No Information 

If we have no data, and the underlying hypothesis is that samples may be put into any of the Groups randomly (e.g., the underlying assumption that all fish species have equal abundance), then the expecation is that the frequencies of observations would be $\frac{1}{K}$, where $K$ is the number of groups.


## Expectations - No Information 

```{r}
n1 <- 52 + 47
n2 <- 32 + 18 
Expected <- matrix( c( 0.5 * n1, 
                       0.5 * n1, 
                       0.5 * n2,
                       0.5 * n2), nrow=2, byrow = TRUE)
rownames( Expected ) <- c("Population 1", "Population 2")
colnames( Expected ) <- c("Catfish", "Other")
Expected
```




## Expecations - Prior Information

In this example, we have a prior expectation that: 

$$
p_{catfish} = 0.74
$$  

$$
p_{other} = 1.0 - 0.74 = 0.26
$$

## Expecations - Prior Information


```{r}
Expected <- matrix( c( 0.74 * n1, 
                       (1-0.74) * n1, 
                       0.74 * n2,
                       (1-0.74) * n2), nrow=2, byrow = TRUE)
rownames( Expected ) <- c("Population 1", "Population 2")
colnames( Expected ) <- c("Catfish", "Other")
Expected
```



## Observations and Expectations

The test statistic should measure how close the data in these two matrices are.

&nbsp;

:::: {.columns}

::: {.column width="50%"}
```{r}
Observed
```
:::

::: {.column width="50%"}
```{r}
Expected
```
:::

::::

&nbsp;

- Small deviations $\to$ no differences and failure to reject $H_O$.  
- Big deviations $\to$ rejection of $H_O$.


## Test Statistic

This is based upon an approximation to the $\chi^2$ distribution, which is well behaved and described for counting data.

&nbsp;

$$
T = \sum_{i=1}^r\sum_{j=1}^c\frac{(O_{ij} - E_{ij})^2}{E_{ij}}
$$


## Estimating Directly 

I'm going to show you how to estimate this from the raw count data so we can get an example where you have to look up the probability for an underlying distribution.  There is a `chisq.test()` function, but you have to be *very careful* that you have it set up properly for contingency table analyses.

::: {.fragment}

```{r}
T <- sum( (Observed - Expected)^2 / Expected  )
T
```

:::

## Rejecting the Null Hypothesis

The general contingency table approach is based upon $df = (r-1)(c-1)$ independent elements and is distributed as a $\chi^2$ random variable.  In a classic 'hypothesis testing' framework with $\alpha = 0.05$, we would estimate the critical value of the test statistic as:

```{r}
alpha <- 0.05
p <- 1.0 - alpha 
critical_value <- qchisq(p, df=1 )
critical_value
```



## The Probability of the Test Statistic

To get the actual probabilty associated with $T =$`r format(T, digits=4)`, we pass the test statistic to the quantile function for the $\chi^2$ distribution.

```{r}
p.value <- 1.0 - pchisq( T, df=1 )
p.value
```


## Tabulating Data

```{r, echo=FALSE}
df <- data.frame( Sample = 1:149,
                  Species = factor( c( rep("Catfish", 52), rep("Other", 47), 
                                       rep("Catfish", 32), rep("Other",18) ) ),
                  Site = factor( c( rep("Population 1", 99), rep("Population 2", 50))),
                  Measurement = rnorm(149,mean=12, sd=3))
head(df)
```


## Tabulating Data 

The `table()` function can take the columns of factors and provide for you the observed matrix of counts.  If you name your factors descriptively, it will take care of the row and column headers for you.

```{r}
table( df$Site, df$Species ) -> Obs
Obs
```



# Non-Parametric Equivallence {background-color="black" background-image="media/contour.png" background-size="initial" background-position="right"}


# What do we do when our data are not normal or heteroscedastistic?



## Non-Parametric Analyses

There are equivalent tests (ok, not equivallent but can be used to answer a similar question) that do not have assumptions of normality, homoscedasticity, etc. 

::: {.fragment}

::: {.callout-important}
These are largely based upon Ranks and Combinatory Theory.  

You gain the applicability with a *potential loss* of information (and hence statistical power).
:::
::: 



## Non-Parametric Correlation 

The normal `cor.test()` function has an option to swap out the Pearson-Product Moment approach for one based on Ranks.     

![](media/cor.test_signature.png)


## Example Comparison

I'm going to use the *Iris* data set to look at a correlation for relatively poorly behaved data.

```{r echo=FALSE}
iris |>
  ggplot( aes(Sepal.Length, Sepal.Width) ) + 
  geom_point( aes(color=Species))  + 
  stat_smooth( method="lm", formula = "y~x", se=FALSE )
```



## Example Comparison

```{r}
fit_pearson <- cor.test( iris$Sepal.Length, iris$Sepal.Width )
fit_kendall <- cor.test( iris$Sepal.Length, iris$Sepal.Width, method = "kendall" )
fit_spearman <- cor.test( iris$Sepal.Length, iris$Sepal.Width, method = "spearman" )
data.frame( Model=c("Pearson","Kendall","Spearman"), 
            Parameter = c("r","tau","rho"),
            R = c(fit_pearson$estimate, fit_kendall$estimate, fit_spearman$estimate),
            P = c(fit_pearson$p.value, fit_kendall$p.value, fit_spearman$p.value ) ) -> df 
```

## Example Comparison

Here is the difference in the models (the ties impacted the Spearman p.value estimate).

```{r}
df |>
  kable( digits = 3, row.names = FALSE) |> 
  kable_classic(  full_width=FALSE )
```






## Non-Parametric Regression 

There are a few different methods for estimating linear regression models via non-parametric approaches.  We've seen one common approach whenever we used the `stat_smooth()` function in `ggplot()`.  By default, it uses the `loess()` local smoother approach.


```{r}
iris |> 
  ggplot( aes(Sepal.Length, Sepal.Width) ) + 
  geom_point() + 
  stat_smooth()
```

## Difference from Least Cost

In a `lm()`, the goal is to use least cost approaches to minimize the error variance across all the data simultaneously.  

![](media/lm_error.png)


## **LO**cal regr**ESS**ion 

The `loess()` approach looks at a local window for estimating the formula of the line along with a moderatly high-dimensional polynomial.

$$
\hat{y} = \hat{\beta}_O^{x} + \hat{\beta}_1^{x}x_i
$$

The function has a signature similar to that we saw in `lm()` for normal [regression](https://dyerlabteaching.github.io/Regression/narrative.html).  


## lowess Signature

![](media/loess_signature.png)

Notice the optional value for `span`, which defines the size of the local window.

## Local Span Size

```{r echo=FALSE}
tricube <- function(x, delta = 0.1) {
  ifelse(abs(x) < delta, (1 - abs(x)^3)^3, 0)
}
set.seed(1)
n <- 101
x <- seq(0, 1, length.out = n)
fx <- sin(2 * pi * x)
y <- fx + rnorm(n, sd = 0.5)

# define x* and color for window
xstar <- 0.3
cols <- rgb(190/255,190/255,190/255,alpha=0.5)

# set-up 2 x 2 subplot
par(mfrow = c(2,2))

# loop through spans (0.1, 0.2, 0.3, 0.4)
for(s in c(0.1, 0.2, 0.3, 0.4)){
  
  # plot data and true function
  plot(x, y, main = paste0("span = ", s), ylim = c(-2.5, 2.5),
       cex.lab = 1.5, cex.axis = 1.25)
  lines(x, fx, col = "blue", lwd = 2)
  
  # plot window
  window <- c(xstar - s / 2, xstar + s / 2)
  rect(window[1], -3, window[2], 3, col = cols)
  
  # define weights
  w <- tricube(x - xstar, delta = s / 2)
  
  # plot estimate
  X <- cbind(1, x - 0.5, (x - 0.5)^2)
  X.w <- sqrt(w) * X
  y.w <- sqrt(w) * y
  beta <- solve(crossprod(X.w)) %*% crossprod(X.w, y.w)
  ystar <- as.numeric(cbind(1, xstar - 0.5, (xstar - 0.5)^2) %*% beta)
  points(xstar, ystar, pch = 17, col = "red", cex = 1)
  
  # add regression line
  lines(x, X %*% beta, lty = 4, col="orange")
  
  # add legend
  legend("topright", legend = c("data", "truth"),
         pch = c(1, NA), lty = c(NA, 1), col = c("black", "blue"), bty = "n")
  legend("bottomright", legend = c("estimate", "window"),
         pch = c(17, 15), col = c("red", "gray"), bty = "n")
  
}
```


## Fitting A loess Model

```{r}
fit <- loess( Sepal.Width ~ Sepal.Length, data=iris)
fit
```


## Examine the Span Parameter

Let's look at the consequence of the span parameter.

```{r}
loess.spans <- function(x, y, s){
  nobs <- length(y)
  xs <- sort(x, index.return = TRUE)
  x <- xs$x
  y <- y[xs$ix]
  lo <- loess(y ~ x, span = s)
  data.frame(Sepal.Length = x, Sepal.Width = lo$fitted, span = s)
}
```

## Plotting Different Spans

```{r echo=FALSE }
rbind( loess.spans( iris$Sepal.Length, iris$Sepal.Width, s = 0.2 ),
       loess.spans( iris$Sepal.Length, iris$Sepal.Width, s = 0.3 ),
       loess.spans( iris$Sepal.Length, iris$Sepal.Width, s = 0.4 ),
       loess.spans( iris$Sepal.Length, iris$Sepal.Width, s = 0.5 ),
       loess.spans( iris$Sepal.Length, iris$Sepal.Width, s = 0.6 ),
       loess.spans( iris$Sepal.Length, iris$Sepal.Width, s = 0.7 ),
       loess.spans( iris$Sepal.Length, iris$Sepal.Width, s = 0.8 ) )  |> 
  mutate( span = factor( span, ordered=TRUE )) |>
  ggplot( aes(Sepal.Length, Sepal.Width) ) + 
  geom_point( data = iris, ) + 
  geom_line( aes(color = span ), lwd=1)  
```

## Optimizing For Span

One method suggested to figure out what an optimal value of `span` would be is based upon a generalized cross validation (GCV) approach.  

$$
s = \frac{ \frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_{i,s})^2 }{ (1 - \frac{v_s}{n} )^2 }
$$

where $v_s = \sum_{i=1}^N h_{ii,s}$ is the trace of the idempotent $\mathbf{H}$ [Hat Matrix](https://www.sciencedirect.com/topics/mathematics/hat-matrix).

## A General Loess GCV 
Here is a function that does this for the general loess model.

```{r}
loess.gcv <- function(x, y){
  nobs <- length(y)
  xs <- sort(x, index.return = TRUE)
  x <- xs$x
  y <- y[xs$ix]
  tune.loess <- function(s){
    lo <- loess(y ~ x, span = s)
    mean((lo$fitted - y)^2) / (1 - lo$trace.hat/nobs)^2
  }
  os <- optimize(tune.loess, interval = c(.01, 99))$minimum
  lo <- loess(y ~ x, span = os)
  data.frame(Sepal.Length = x, Sepal.Width = lo$fitted, span = os)
}
```


## Plot 

```{r}
rbind( loess.gcv( iris$Sepal.Length, iris$Sepal.Width ),
       loess.spans( iris$Sepal.Length, iris$Sepal.Width, s = 0.75 ) ) |> 
  mutate( span = factor(round(span,digits = 2))) |>
  ggplot( aes(Sepal.Length, Sepal.Width, color=span) ) + 
  geom_line() 
```


## Other Regression Approaches

There are several additional approaches including Kernel Regression (e.g., [Nadaraya & Watson's Kernel Smoother](https://bookdown.org/egarpor/PM-UC3M/npreg-kre.html)) and Local Averaging (e.g., [Friedman's Super Smoother](http://fmwww.bc.edu/repec/bocode/s/supsmooth_doc.pdf)).  If you need to perform a regression approach, please explore the benefits and challenges with these alternative methods before deciding on which one to use.





## Non-Parametric T-Test: Wilcoxon Test


This is an extension of the `t-test` for the equivalence of the mean values of two data sets (e.g., $H_O: \bar{x} = \bar{y}$).  


## Wilcoxon Test Assumptions

This approach has the following assumptions:

1. Both sets of data are random samples from their respective population.  
2. Each of the populations are similarly independent.
3. The measurement scale for each variable is at least ordinal (e.g., you can put them in order).  


## Example Data 

```{r}
mtcars |>
  select( Transmission = am, MPG = mpg ) |>
  mutate( Transmission = as.factor( Transmission ) ) |>
  mutate( Transmission = fct_recode( Transmission, "Automatic"="0", "Manual"="1")) -> data

data |>
  ggplot( aes(MPG, fill=Transmission) )  + 
  geom_density( alpha = 0.5 )
```


## Wilcoxon Test 

To test this, we use the Wilcoxon Rank Sum test.  The signature of this test is:

![](media/Wilcoxon_signature.png) 


## Wilcoxon Test

This will take the full set of data, assign them ranked values, and then determine if the ranked values are randomly distributed between the two groups.

```{r}
fit <- wilcox.test( MPG ~ Transmission, data=data)
fit 
```


## Contained Values


As for other analyses, the object returned has a number of elements that may be helpful in writing about the analysis.

```{r}
names(fit)
```




## Non-Parametric ANOVA {.smaller}

The Kruskal-Wallis test is an extension on the Wilcoxon test in the same way that the `aov()` is an extension of the `t-test()`.  The approach is again based upon ranks. Here is an example data set we've used once before based on air quality measured across five different months.

```{r}
airquality |>
  select( Month, Ozone ) |> 
  mutate( Month = factor( Month ) ) |> 
  mutate( Month = fct_recode(Month, 
                             May="5",
                             Jun="6",
                             Jul="7",
                             Aug="8",
                             Sep="9" )) |> 
  filter( !is.na( Ozone )) -> df.air 
```


## Kruskal-Wallis Data

```{r}
df.air |>
  ggplot(aes(Month, Ozone) ) + 
  geom_boxplot(notch=TRUE)
```

## Kruskal-Wallis Tests 

To test for the equality of ozone across months, we use `kruskal.test()` 

```{r}
fit <- kruskal.test( Ozone ~ Month, data=df.air )
fit 
```

## Contained Values


As for other analyses, the object returned has a number of elements that may be helpful in writing about the analysis.

```{r}
names(fit)
```



## Questions?

::: {layout-ncol="2"}
If you have any questions, please feel free to post to the [Canvas](https://canvas.vcu.edu) discussion board for the class, or drop me an [email](mailto://rjdyer@vcu.edu).

![](media/peter_sellers.gif){.middle fig-alt="Peter Sellers looking bored" fig-align="center" width="500"}
:::

















